{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9fc406",
   "metadata": {},
   "source": [
    "### Great work team!👏 👏 👏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f0544-a9e4-452a-8912-62da8b85e5fa",
   "metadata": {},
   "source": [
    "# Anostep results so far (updated daily)\n",
    "\n",
    "Here are some quick analyses to look at the data so far :)\n",
    "We start by accessing the data and cleaning it up\n",
    "The top code is the simplified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e507e9-e0cb-40ba-a4a7-14ac5e8dea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to CommCare OData...\n",
      "✅ Successfully retrieved 1,229 records with 59 columns\n",
      "🧹 Cleaned 51 column names\n",
      "💾 Data saved to commcare_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Clean up column names by removing 'form ' prefix and replacing special characters\"\"\"\n",
    "    column_mapping = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        new_col = col\n",
    "        \n",
    "        # Remove \"form \" prefix\n",
    "        if col.lower().startswith('form '):\n",
    "            new_col = col[5:]\n",
    "        \n",
    "        # Clean up special characters\n",
    "        new_col = new_col.replace(' | ', '_').replace(': ', '_').replace(' ', '_')\n",
    "        new_col = new_col.replace('__', '_').strip('_')\n",
    "        \n",
    "        column_mapping[col] = new_col\n",
    "    \n",
    "    return df.rename(columns=column_mapping), column_mapping\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    url = \"https://www.commcarehq.org/a/atsb-project-1/api/odata/forms/v1/b81194d6348b69e2ce1880689777a556/feed\"\n",
    "    username = \"otienobrn09@gmail.com\"\n",
    "    password = \"Tracy@2013\"\n",
    "    \n",
    "    print(\"Connecting to CommCare OData...\")\n",
    "    \n",
    "    try:\n",
    "        # Make API request\n",
    "        response = requests.get(url, auth=HTTPBasicAuth(username, password), timeout=30)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ Error {response.status_code}: {response.reason}\")\n",
    "            return\n",
    "        \n",
    "        # Process data\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data.get('value'):\n",
    "            print(\"❌ No data found\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data['value'])\n",
    "        \n",
    "        # Clean column names\n",
    "        df, column_mapping = clean_column_names(df)\n",
    "        renamed_count = len([k for k, v in column_mapping.items() if k != v])\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"✅ Successfully retrieved {len(df):,} records with {len(df.columns)} columns\")\n",
    "        if renamed_count > 0:\n",
    "            print(f\"🧹 Cleaned {renamed_count} column names\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        filename = \"commcare_data.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"💾 Data saved to {filename}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Request failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da63bc5-5133-488b-b8b2-383e2a1c8ffe",
   "metadata": {},
   "source": [
    "This code will now sync the data every day at 13hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe498ed-1e10-4149-8255-b529dba10131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Auto-sync scheduler started!\n",
      "📅 Daily sync scheduled at 13:15 (1 PM)\n",
      "⚡ Running initial sync now...\n",
      "\n",
      "🕐 Starting sync at 2025-10-03 15:19:12\n",
      "Connecting to CommCare OData...\n",
      "💾 All data saved to commcare_data.csv\n",
      "🗑️ Removed 36 rows where anoph_present = '---'\n",
      "💾 Cleaned data saved to commcare_cleaned_data.csv\n",
      "✅ Successfully processed:\n",
      "   📊 All data: 1,220 records with 59 columns\n",
      "   🧽 Cleaned data: 1,184 records with 59 columns\n",
      "🧹 Cleaned 51 column names\n",
      "✅ Sync completed at 2025-10-03 15:19:16\n",
      "\n",
      "⏰ Next sync scheduled for today at 13:15\n",
      "🔄 Scheduler running... Press Ctrl+C to stop\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Clean up column names by removing 'form ' prefix and replacing special characters\"\"\"\n",
    "    column_mapping = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        new_col = col\n",
    "        \n",
    "        # Remove \"form \" prefix\n",
    "        if col.lower().startswith('form '):\n",
    "            new_col = col[5:]\n",
    "        \n",
    "        # Clean up special characters\n",
    "        new_col = new_col.replace(' | ', '_').replace(': ', '_').replace(' ', '_')\n",
    "        new_col = new_col.replace('__', '_').strip('_')\n",
    "        \n",
    "        column_mapping[col] = new_col\n",
    "    \n",
    "    return df.rename(columns=column_mapping), column_mapping\n",
    "\n",
    "def sync_data():\n",
    "    \"\"\"Main data sync function\"\"\"\n",
    "    print(f\"\\n🕐 Starting sync at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Configuration\n",
    "    url = \"https://www.commcarehq.org/a/atsb-project-1/api/odata/forms/v1/b81194d6348b69e2ce1880689777a556/feed\"\n",
    "    username = \"otienobrn09@gmail.com\"\n",
    "    password = \"Tracy@2013\"\n",
    "    \n",
    "    print(\"Connecting to CommCare OData...\")\n",
    "    \n",
    "    try:\n",
    "        # Make API request\n",
    "        response = requests.get(url, auth=HTTPBasicAuth(username, password), timeout=30)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ Error {response.status_code}: {response.reason}\")\n",
    "            return\n",
    "        \n",
    "        # Process data\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data.get('value'):\n",
    "            print(\"❌ No data found\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data['value'])\n",
    "        \n",
    "        # Clean column names\n",
    "        df, column_mapping = clean_column_names(df)\n",
    "        renamed_count = len([k for k, v in column_mapping.items() if k != v])\n",
    "        \n",
    "        # Save all data (unfiltered)\n",
    "        all_data_filename = \"commcare_data.csv\"\n",
    "        df.to_csv(all_data_filename, index=False)\n",
    "        print(f\"💾 All data saved to {all_data_filename}\")\n",
    "        \n",
    "        # Filter out rows where anoph_present is '---' for cleaned dataset\n",
    "        initial_rows = len(df)\n",
    "        df_cleaned = df.copy()\n",
    "        if 'anoph_present' in df.columns:\n",
    "            df_cleaned = df_cleaned[df_cleaned['anoph_present'] != '---']\n",
    "            filtered_rows = initial_rows - len(df_cleaned)\n",
    "            if filtered_rows > 0:\n",
    "                print(f\"🗑️ Removed {filtered_rows} rows where anoph_present = '---'\")\n",
    "        \n",
    "        # Save cleaned data\n",
    "        cleaned_filename = \"commcare_cleaned_data.csv\"\n",
    "        df_cleaned.to_csv(cleaned_filename, index=False)\n",
    "        print(f\"💾 Cleaned data saved to {cleaned_filename}\")\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"✅ Successfully processed:\")\n",
    "        print(f\"   📊 All data: {len(df):,} records with {len(df.columns)} columns\")\n",
    "        print(f\"   🧽 Cleaned data: {len(df_cleaned):,} records with {len(df_cleaned.columns)} columns\")\n",
    "        if renamed_count > 0:\n",
    "            print(f\"🧹 Cleaned {renamed_count} column names\")\n",
    "        \n",
    "        print(f\"✅ Sync completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Request failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Schedule daily sync at 13:15 (1 PM)\n",
    "    schedule.every().day.at(\"13:15\").do(sync_data)\n",
    "    \n",
    "    print(\"🔄 Auto-sync scheduler started!\")\n",
    "    print(\"📅 Daily sync scheduled at 13:15 (1 PM)\")\n",
    "    print(\"⚡ Running initial sync now...\")\n",
    "    \n",
    "    # Run initial sync\n",
    "    sync_data()\n",
    "    \n",
    "    print(f\"\\n⏰ Next sync scheduled for today at 13:15\")\n",
    "    print(\"🔄 Scheduler running... Press Ctrl+C to stop\")\n",
    "    \n",
    "    # Keep the scheduler running\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(60)  # Check every minute\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41928fe-a18f-4f4b-b1d3-10596f981ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def load_credentials():\n",
    "    \"\"\"Load credentials - hardcoded for now\"\"\"\n",
    "    username = \"otienobrn09@gmail.com\"\n",
    "    password = \"Tracy@2013\"\n",
    "    \n",
    "    return username, password\n",
    "\n",
    "def fetch_commcare_data(url, username, password, timeout=30):\n",
    "    \"\"\"Fetch data from CommCare OData API\"\"\"\n",
    "    print(\"Connecting to CommCare OData...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            auth=HTTPBasicAuth(username, password),\n",
    "            timeout=timeout,\n",
    "            headers={'Accept': 'application/json'}\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json(), None\n",
    "        else:\n",
    "            error_messages = {\n",
    "                401: \"Authentication failed. Please check your credentials.\",\n",
    "                403: \"Access forbidden. You may not have permission to access this data.\",\n",
    "                404: \"Data source not found. Please check the URL.\",\n",
    "                429: \"Too many requests. Please wait and try again.\",\n",
    "                500: \"Server error. Please try again later.\"\n",
    "            }\n",
    "            \n",
    "            error_msg = error_messages.get(\n",
    "                response.status_code, \n",
    "                f\"HTTP Error {response.status_code}: {response.reason}\"\n",
    "            )\n",
    "            return None, error_msg\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        return None, \"Request timed out. The server may be slow to respond.\"\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return None, \"Connection error. Please check your internet connection.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None, f\"Request failed: {e}\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Unexpected error: {e}\"\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Remove 'form ' prefix from column names and clean them up\"\"\"\n",
    "    \n",
    "    # Create a mapping of old to new column names\n",
    "    column_mapping = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        new_col = col\n",
    "        \n",
    "        # Remove \"form \" prefix (case insensitive)\n",
    "        if col.lower().startswith('form '):\n",
    "            new_col = col[5:]  # Remove first 5 characters (\"form \")\n",
    "        \n",
    "        # Clean up other common patterns\n",
    "        new_col = new_col.replace(' | ', '_')  # Replace \" | \" with \"_\"\n",
    "        new_col = new_col.replace(': ', '_')   # Replace \": \" with \"_\"\n",
    "        new_col = new_col.replace(' ', '_')    # Replace spaces with underscores\n",
    "        new_col = new_col.replace('__', '_')   # Replace double underscores with single\n",
    "        new_col = new_col.strip('_')           # Remove leading/trailing underscores\n",
    "        \n",
    "        column_mapping[col] = new_col\n",
    "    \n",
    "    # Rename the columns\n",
    "    df_clean = df.rename(columns=column_mapping)\n",
    "    \n",
    "    return df_clean, column_mapping\n",
    "\n",
    "def analyze_dataframe(df):\n",
    "    \"\"\"Analyze and display basic DataFrame information\"\"\"\n",
    "    print(f\"\\n✅ Successfully retrieved data!\")\n",
    "    print(f\"📊 Total records: {len(df):,}\")\n",
    "    print(f\"📋 Columns: {len(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_data_info(df):\n",
    "    \"\"\"Display basic data information\"\"\"\n",
    "    print(f\"\\n📊 Data Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n",
    "def save_data(df, base_filename=\"commcare_data\"):\n",
    "    \"\"\"Save data as CSV file\"\"\"\n",
    "    filename = f\"{base_filename}.csv\"\n",
    "    \n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    size_mb = os.path.getsize(filename) / 1024 / 1024\n",
    "    print(f\"\\n✅ Data saved to: {filename}\")\n",
    "    print(f\"📁 File size: {size_mb:.2f} MB\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the data retrieval process\"\"\"\n",
    "    # Configuration\n",
    "    url = \"https://www.commcarehq.org/a/atsb-project-1/api/odata/forms/v1/b81194d6348b69e2ce1880689777a556/feed\"\n",
    "    \n",
    "    # Load credentials securely\n",
    "    username, password = load_credentials()\n",
    "    \n",
    "    # Fetch data\n",
    "    data, error = fetch_commcare_data(url, username, password)\n",
    "    \n",
    "    if error:\n",
    "        print(f\"❌ {error}\")\n",
    "        return\n",
    "    \n",
    "    # Check if data contains records\n",
    "    if not data or 'value' not in data or not data['value']:\n",
    "        print(\"❌ No data found in the response\")\n",
    "        if data:\n",
    "            print(\"Response structure:\", list(data.keys()) if isinstance(data, dict) else type(data))\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    try:\n",
    "        df = pd.DataFrame(data['value'])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating DataFrame: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Clean column names\n",
    "    df, column_mapping = clean_column_names(df)\n",
    "    if column_mapping:\n",
    "        renamed_count = len([k for k, v in column_mapping.items() if k != v])\n",
    "        if renamed_count > 0:\n",
    "            print(f\"🧹 Cleaned {renamed_count} column names\")\n",
    "    \n",
    "    # Analyze and display data\n",
    "    df = analyze_dataframe(df)\n",
    "    display_data_info(df)\n",
    "    \n",
    "    # Save data\n",
    "    save_data(df)\n",
    "    \n",
    "    print(f\"\\n🔚 Script completed successfully!\")\n",
    "    print(f\"📈 Final dataset: {len(df):,} rows × {len(df.columns)} columns\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea369f-69e6-416f-863a-2dea69be86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import threading\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "url = \"https://www.commcarehq.org/a/atsb-project-1/api/odata/forms/v1/b81194d6348b69e2ce1880689777a556/feed\"\n",
    "username = \"otienobrn09@gmail.com\"\n",
    "password = \"Tracy@2013\"\n",
    "\n",
    "# Setup logging\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    log_filename = f\"commcare_sync_{datetime.now().strftime('%Y%m%d')}.log\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Remove 'form ' prefix from column names and clean them up\"\"\"\n",
    "    \n",
    "    # Create a mapping of old to new column names\n",
    "    column_mapping = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        new_col = col\n",
    "        \n",
    "        # Remove \"form \" prefix (case insensitive)\n",
    "        if col.lower().startswith('form '):\n",
    "            new_col = col[5:]  # Remove first 5 characters (\"form \")\n",
    "        \n",
    "        # Clean up other common patterns\n",
    "        new_col = new_col.replace(' | ', '_')  # Replace \" | \" with \"_\"\n",
    "        new_col = new_col.replace(': ', '_')   # Replace \": \" with \"_\"\n",
    "        new_col = new_col.replace(' ', '_')    # Replace spaces with underscores\n",
    "        new_col = new_col.replace('__', '_')   # Replace double underscores with single\n",
    "        new_col = new_col.strip('_')           # Remove leading/trailing underscores\n",
    "        \n",
    "        column_mapping[col] = new_col\n",
    "    \n",
    "    # Rename the columns\n",
    "    df_clean = df.rename(columns=column_mapping)\n",
    "    \n",
    "    return df_clean, column_mapping\n",
    "\n",
    "# def fetch_all_data_auto(max_records=10000, batch_size=100):\n",
    "#     \"\"\"Automatically fetch all available data with deduplication\"\"\"\n",
    "#     auth = HTTPBasicAuth(username, password)\n",
    "#     all_data = []\n",
    "#     seen_ids = set()  # Track unique identifiers to prevent duplicates\n",
    "#     offset = 0\n",
    "#     consecutive_empty_batches = 0\n",
    "#     max_empty_batches = 3  # Stop after 3 consecutive empty batches\n",
    "    \n",
    "#     logger.info(f\"Starting automatic data fetch - max {max_records} records\")\n",
    "    \n",
    "#     while len(all_data) < max_records and consecutive_empty_batches < max_empty_batches:\n",
    "#         try:\n",
    "#             params = {\n",
    "#                 '$top': batch_size,\n",
    "#                 '$skip': offset\n",
    "#             }\n",
    "            \n",
    "#             logger.info(f\"Fetching batch: records {offset} to {offset + batch_size}\")\n",
    "            \n",
    "#             response = requests.get(url, auth=auth, params=params, timeout=60)\n",
    "            \n",
    "#             if response.status_code == 401:\n",
    "#                 logger.error(\"Authentication failed - check credentials\")\n",
    "#                 break\n",
    "#             elif response.status_code == 403:\n",
    "#                 logger.error(\"Access forbidden - check permissions\")\n",
    "#                 break\n",
    "#             elif response.status_code != 200:\n",
    "#                 logger.error(f\"HTTP {response.status_code} at offset {offset}\")\n",
    "#                 break\n",
    "            \n",
    "#             data = response.json()\n",
    "            \n",
    "#             if 'value' not in data:\n",
    "#                 logger.error(\"No 'value' key in response\")\n",
    "#                 break\n",
    "                \n",
    "#             batch_data = data['value']\n",
    "            \n",
    "#             if not batch_data:\n",
    "#                 consecutive_empty_batches += 1\n",
    "#                 logger.info(f\"Empty batch {consecutive_empty_batches}/{max_empty_batches} - checking for more data\")\n",
    "#                 offset += batch_size\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 consecutive_empty_batches = 0  # Reset counter when we get data\n",
    "            \n",
    "#             # Deduplicate based on a unique identifier\n",
    "#             new_records = []\n",
    "#             duplicates_found = 0\n",
    "            \n",
    "#             for record in batch_data:\n",
    "#                 # Try to find a unique identifier - adjust these field names based on your data structure\n",
    "#                 unique_id = None\n",
    "                \n",
    "#                 # Common unique identifier fields in CommCare - try these in order\n",
    "#                 for id_field in ['id', '_id', 'caseid', 'form_id', 'instanceID', '__id__']:\n",
    "#                     if id_field in record and record[id_field]:\n",
    "#                         unique_id = str(record[id_field])\n",
    "#                         break\n",
    "                \n",
    "#                 # If no standard ID field found, create a hash of the entire record\n",
    "#                 if unique_id is None:\n",
    "#                     # Sort the record items to ensure consistent hashing\n",
    "#                     record_str = json.dumps(record, sort_keys=True)\n",
    "#                     unique_id = str(hash(record_str))\n",
    "#                     logger.warning(f\"No standard ID field found, using hash: {unique_id}\")\n",
    "                \n",
    "#                 if unique_id not in seen_ids:\n",
    "#                     seen_ids.add(unique_id)\n",
    "#                     new_records.append(record)\n",
    "#                 else:\n",
    "#                     duplicates_found += 1\n",
    "            \n",
    "#             if duplicates_found > 0:\n",
    "#                 logger.warning(f\"Found {duplicates_found} duplicate records in this batch\")\n",
    "            \n",
    "#             all_data.extend(new_records)\n",
    "#             logger.info(f\"Batch complete. New unique records: {len(new_records)}, Total records: {len(all_data)}\")\n",
    "            \n",
    "#             # Check if we got fewer records than requested (potential end of data)\n",
    "#             if len(batch_data) < batch_size:\n",
    "#                 logger.info(\"Got fewer records than requested - might be reaching end of data\")\n",
    "#                 # Still continue to check a few more batches in case there are gaps\n",
    "            \n",
    "#             offset += batch_size\n",
    "            \n",
    "#             # Add a small delay to be respectful to the API\n",
    "#             time.sleep(0.1)\n",
    "            \n",
    "#         except requests.exceptions.Timeout:\n",
    "#             logger.error(f\"Request timeout at offset {offset}\")\n",
    "#             break\n",
    "#         except requests.exceptions.ConnectionError:\n",
    "#             logger.error(f\"Connection error at offset {offset}\")\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error fetching batch at offset {offset}: {e}\")\n",
    "#             import traceback\n",
    "#             logger.error(traceback.format_exc())\n",
    "#             break\n",
    "    \n",
    "#     logger.info(f\"Data fetch completed. Total unique records: {len(all_data)}\")\n",
    "    \n",
    "#     if all_data:\n",
    "#         df = pd.DataFrame(all_data)\n",
    "        \n",
    "#         # Additional pandas-level deduplication as a safety measure\n",
    "#         initial_count = len(df)\n",
    "        \n",
    "#         # Try to find the best column for deduplication\n",
    "#         id_columns = ['id', '_id', 'caseid', 'form_id', 'instanceID', '__id__']\n",
    "#         dedup_column = None\n",
    "        \n",
    "#         for col in id_columns:\n",
    "#             if col in df.columns:\n",
    "#                 dedup_column = col\n",
    "#                 break\n",
    "        \n",
    "#         if dedup_column:\n",
    "#             df = df.drop_duplicates(subset=[dedup_column])\n",
    "#             final_count = len(df)\n",
    "#             if initial_count != final_count:\n",
    "#                 logger.warning(f\"Removed {initial_count - final_count} additional duplicates using column '{dedup_column}'\")\n",
    "#         else:\n",
    "#             # If no ID column found, remove duplicates based on all columns\n",
    "#             df = df.drop_duplicates()\n",
    "#             final_count = len(df)\n",
    "#             if initial_count != final_count:\n",
    "#                 logger.warning(f\"Removed {initial_count - final_count} additional duplicates using all columns\")\n",
    "        \n",
    "#         return df\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "def save_daily_data(df_clean, column_mapping):\n",
    "    \"\"\"Save daily data with timestamp\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save cleaned data with timestamp\n",
    "    data_filename = f\"commcare_data_{timestamp}.csv\"\n",
    "    df_clean.to_csv(data_filename, index=False)\n",
    "    logger.info(f\"Daily data saved to '{data_filename}'\")\n",
    "    \n",
    "    # Always maintain current files for other scripts\n",
    "    current_data_filename = \"commcare_cleaned_data.csv\"\n",
    "    df_clean.to_csv(current_data_filename, index=False)\n",
    "    logger.info(f\"Current data updated: '{current_data_filename}'\")\n",
    "    \n",
    "    # Save column mapping\n",
    "    mapping_df = pd.DataFrame(list(column_mapping.items()), \n",
    "                             columns=['Original_Column', 'Cleaned_Column'])\n",
    "    mapping_filename = f\"commcare_column_mapping_{timestamp}.csv\"\n",
    "    mapping_df.to_csv(mapping_filename, index=False)\n",
    "    \n",
    "    current_mapping_filename = \"commcare_column_mapping.csv\"\n",
    "    mapping_df.to_csv(current_mapping_filename, index=False)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'sync_timestamp': timestamp,\n",
    "        'sync_datetime': datetime.now().isoformat(),\n",
    "        'total_records': len(df_clean),\n",
    "        'total_columns': len(df_clean.columns),\n",
    "        'columns': list(df_clean.columns),\n",
    "        'data_types': {col: str(dtype) for col, dtype in df_clean.dtypes.items()},\n",
    "        'null_counts': {col: int(df_clean[col].isnull().sum()) for col in df_clean.columns}\n",
    "    }\n",
    "    \n",
    "    metadata_filename = f\"commcare_metadata_{timestamp}.json\"\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    current_metadata_filename = \"commcare_metadata.json\"\n",
    "    with open(current_metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Metadata saved to '{metadata_filename}'\")\n",
    "    \n",
    "    return {\n",
    "        'data_file': data_filename,\n",
    "        'current_data_file': current_data_filename,\n",
    "        'mapping_file': mapping_filename,\n",
    "        'metadata_file': metadata_filename,\n",
    "        'records_count': len(df_clean)\n",
    "    }\n",
    "\n",
    "# def daily_sync_job():\n",
    "#     \"\"\"Daily synchronization job\"\"\"\n",
    "#     logger.info(\"=\" * 60)\n",
    "#     logger.info(\"DAILY COMMCARE DATA SYNC STARTED\")\n",
    "#     logger.info(\"=\" * 60)\n",
    "    \n",
    "#     try:\n",
    "#         # Test connection first\n",
    "#         auth = HTTPBasicAuth(username, password)\n",
    "#         test_response = requests.get(url, auth=auth, timeout=30, params={'$top': 1})\n",
    "        \n",
    "#         if test_response.status_code != 200:\n",
    "#             logger.error(f\"Connection test failed: HTTP {test_response.status_code}\")\n",
    "#             return\n",
    "        \n",
    "#         logger.info(\"Connection test successful\")\n",
    "        \n",
    "#         # Fetch all data\n",
    "#         df = fetch_all_data_auto()\n",
    "        \n",
    "#         if df is None:\n",
    "#             logger.error(\"Failed to fetch data from CommCare\")\n",
    "#             return\n",
    "        \n",
    "#         logger.info(f\"Successfully fetched {len(df)} records with {len(df.columns)} columns\")\n",
    "        \n",
    "#         # Clean column names\n",
    "#         logger.info(\"Cleaning column names...\")\n",
    "#         df_clean, column_mapping = clean_column_names(df)\n",
    "        \n",
    "#         # Save data\n",
    "#         logger.info(\"Saving data...\")\n",
    "#         files = save_daily_data(df_clean, column_mapping)\n",
    "        \n",
    "#         logger.info(\"=\" * 60)\n",
    "#         logger.info(\"DAILY SYNC COMPLETED SUCCESSFULLY\")\n",
    "#         logger.info(f\"Records processed: {files['records_count']}\")\n",
    "#         logger.info(f\"Files updated:\")\n",
    "#         for desc, filename in files.items():\n",
    "#             if desc.endswith('_file'):\n",
    "#                 logger.info(f\"  • {filename}\")\n",
    "#         logger.info(\"=\" * 60)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Daily sync failed: {e}\")\n",
    "#         import traceback\n",
    "#         logger.error(\"Full error details:\")\n",
    "#         logger.error(traceback.format_exc())\n",
    "\n",
    "def show_status():\n",
    "    \"\"\"Show current sync status\"\"\"\n",
    "    print(\"\\n📊 COMMCARE DATA SYNC STATUS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if current files exist\n",
    "    current_files = {\n",
    "        'Data': 'commcare_cleaned_data.csv',\n",
    "        'Mapping': 'commcare_column_mapping.csv', \n",
    "        'Metadata': 'commcare_metadata.json'\n",
    "    }\n",
    "    \n",
    "    for file_type, filename in current_files.items():\n",
    "        if os.path.exists(filename):\n",
    "            mod_time = datetime.fromtimestamp(os.path.getmtime(filename))\n",
    "            size = os.path.getsize(filename)\n",
    "            print(f\"✅ {file_type:8}: {filename}\")\n",
    "            print(f\"   Last updated: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"   Size: {size:,} bytes\")\n",
    "        else:\n",
    "            print(f\"❌ {file_type:8}: {filename} (not found)\")\n",
    "    \n",
    "    # Check recent log files\n",
    "    log_files = [f for f in os.listdir('.') if f.startswith('commcare_sync_') and f.endswith('.log')]\n",
    "    if log_files:\n",
    "        latest_log = max(log_files, key=lambda f: os.path.getmtime(f))\n",
    "        mod_time = datetime.fromtimestamp(os.path.getmtime(latest_log))\n",
    "        print(f\"\\n📋 Latest log: {latest_log}\")\n",
    "        print(f\"   Last modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Show next scheduled run\n",
    "    jobs = schedule.get_jobs()\n",
    "    if jobs:\n",
    "        next_run = min(job.next_run for job in jobs)\n",
    "        print(f\"\\n⏰ Next scheduled sync: {next_run.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    else:\n",
    "        print(f\"\\n⏰ No scheduled syncs configured\")\n",
    "\n",
    "def main():\n",
    "    print(\"🔄 CommCare Auto-Sync Data Loader\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Automatic daily sync scheduled for 9:15 AM\")\n",
    "    print(\"Press Ctrl+C to stop the scheduler\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Schedule daily job at 9:15 AM\n",
    "    schedule.every().day.at(\"09:15\").do(daily_sync_job)\n",
    "    \n",
    "    # Show initial status\n",
    "    show_status()\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Starting CommCare data scheduler...\")\n",
    "        logger.info(\"Daily sync scheduled for 9:15 AM\")\n",
    "        logger.info(\"Press Ctrl+C to stop\")\n",
    "        \n",
    "        # Run scheduler loop\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)  # Check every minute\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⏹️  Scheduler stopped by user\")\n",
    "        logger.info(\"Scheduler stopped by user\")\n",
    "        print(\"👋 Goodbye!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df4c8d-e1a4-414d-84d7-335067c7fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "url = \"https://www.commcarehq.org/a/atsb-project-1/api/odata/forms/v1/b81194d6348b69e2ce1880689777a556/feed\"\n",
    "username = \"otienobrn09@gmail.com\"\n",
    "password = \"Tracy@2013\"\n",
    "\n",
    "# Simple logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def fetch_and_process_data():\n",
    "    \"\"\"Fetch data from CommCare API, flatten nested structures, and deduplicate by formid\"\"\"\n",
    "    \n",
    "    auth = HTTPBasicAuth(username, password)\n",
    "    all_data = []\n",
    "    offset = 0\n",
    "    batch_size = 100\n",
    "    \n",
    "    logger.info(\"Starting data fetch from CommCare API...\")\n",
    "    \n",
    "    # # Fetch all data in batches\n",
    "    # while True:\n",
    "    #     try:\n",
    "    #         params = {'$top': batch_size, '$skip': offset}\n",
    "    #         response = requests.get(url, auth=auth, params=params, timeout=60)\n",
    "            \n",
    "    #         if response.status_code != 200:\n",
    "    #             logger.error(f\"HTTP {response.status_code} - stopping fetch\")\n",
    "    #             break\n",
    "            \n",
    "    #         data = response.json()\n",
    "    #         batch_data = data.get('value', [])\n",
    "            \n",
    "    #         if not batch_data:\n",
    "    #             logger.info(\"No more data - fetch complete\")\n",
    "    #             break\n",
    "            \n",
    "    #         all_data.extend(batch_data)\n",
    "    #         logger.info(f\"Fetched {len(all_data)} records so far...\")\n",
    "            \n",
    "    #         if len(batch_data) < batch_size:\n",
    "    #             break\n",
    "                \n",
    "    #         offset += batch_size\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         logger.error(f\"Error fetching data: {e}\")\n",
    "    #         break\n",
    "    \n",
    "    # if not all_data:\n",
    "    #     logger.error(\"No data fetched\")\n",
    "    #     return None\n",
    "    \n",
    "    # logger.info(f\"Total raw records: {len(all_data)}\")\n",
    "    \n",
    "    # Flatten nested JSON structures\n",
    "    logger.info(\"Flattening nested data...\")\n",
    "    df = pd.json_normalize(all_data)\n",
    "    \n",
    "    logger.info(f\"After flattening: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Find formid column for deduplication\n",
    "    formid_columns = [col for col in df.columns if 'formid' in col.lower() or 'form_id' in col.lower() or 'instanceid' in col.lower()]\n",
    "    \n",
    "    if formid_columns:\n",
    "        formid_col = formid_columns[0]\n",
    "        logger.info(f\"Using '{formid_col}' for deduplication\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        original_count = len(df)\n",
    "        df = df.drop_duplicates(subset=[formid_col], keep='first')\n",
    "        final_count = len(df)\n",
    "        \n",
    "        logger.info(f\"Deduplicated: {original_count} -> {final_count} records\")\n",
    "    else:\n",
    "        logger.warning(\"No formid column found - skipping deduplication\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Clean up column names\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Rename columns\n",
    "    new_columns = {}\n",
    "    for col in df.columns:\n",
    "        new_col = col\n",
    "        if col.lower().startswith('form '):\n",
    "            new_col = col[5:]  # Remove \"form \" prefix\n",
    "        new_col = new_col.replace(' | ', '_').replace(': ', '_').replace(' ', '_')\n",
    "        new_col = new_col.replace('__', '_').strip('_')\n",
    "        new_columns[col] = new_col\n",
    "    \n",
    "    df = df.rename(columns=new_columns)\n",
    "    return df\n",
    "\n",
    "def save_data(df):\n",
    "    \"\"\"Save the processed data\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save main data file\n",
    "    main_filename = \"commcare_cleaned_data.csv\"\n",
    "    timestamped_filename = f\"commcare_data_{timestamp}.csv\"\n",
    "    \n",
    "    df.to_csv(main_filename, index=False)\n",
    "    df.to_csv(timestamped_filename, index=False)\n",
    "    \n",
    "    logger.info(f\"Data saved to: {main_filename}\")\n",
    "    logger.info(f\"Backup saved to: {timestamped_filename}\")\n",
    "    \n",
    "    return main_filename\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🔄 CommCare Data Fetcher (Simplified)\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Fetching, flattening, and deduplicating CommCare data...\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch and process data\n",
    "        df = fetch_and_process_data()\n",
    "        \n",
    "        if df is None:\n",
    "            print(\"❌ Failed to fetch data\")\n",
    "            return\n",
    "        \n",
    "        # Clean column names\n",
    "        df = clean_column_names(df)\n",
    "        \n",
    "        # Save data\n",
    "        filename = save_data(df)\n",
    "        \n",
    "        # Show results\n",
    "        print(f\"\\n✅ SUCCESS!\")\n",
    "        print(f\"📊 Records: {len(df)}\")\n",
    "        print(f\"📋 Columns: {len(df.columns)}\")\n",
    "        print(f\"📁 Saved to: {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        logger.error(f\"Main execution error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
